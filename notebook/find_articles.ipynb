{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2982ee3",
   "metadata": {},
   "source": [
    "# Tutorial: Extracting Wikipedia Pages & Detecting Redirects\n",
    "\n",
    "Wikipedia provides large datasets of its contents via downloadable dumps. One common format is the .bz2-compressed pages-articles XML. This tutorial shows you how to read and parse this file using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f70df-3e9f-4637-88f5-94fb5e22f955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T10:44:57.128897Z",
     "iopub.status.busy": "2025-04-09T10:44:57.128897Z",
     "iopub.status.idle": "2025-04-09T10:49:40.034403Z",
     "shell.execute_reply": "2025-04-09T10:49:40.034403Z",
     "shell.execute_reply.started": "2025-04-09T10:44:57.128897Z"
    }
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "\n",
    "# Define the path to the dump file - Update this with your path to the dump\n",
    "dump_path = \"D:Users/Paschalis/phd/data/dumps/EN/wikipedia/enwiki-20250123-pages-articles-multistream.xml.bz2\"\n",
    "\n",
    "# Initialize a list to store the results\n",
    "titles_and_texts = []\n",
    "\n",
    "# Open and read the compressed XML file\n",
    "with bz2.open(dump_path, 'rt', encoding='utf-8') as file:\n",
    "    inside_page = False\n",
    "    inside_text_tag = False\n",
    "    title = \"\"\n",
    "    text = []\n",
    "\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line == \"<page>\":\n",
    "            inside_page = True\n",
    "            title = \"\"\n",
    "            text = []\n",
    "            inside_text_tag = False\n",
    "\n",
    "        elif line == \"</page>\":\n",
    "            if title and text:\n",
    "                titles_and_texts.append({\n",
    "                    \"title\": title,\n",
    "                    \"text\": '\\n'.join(text).strip()\n",
    "                })\n",
    "            inside_page = False\n",
    "\n",
    "        elif inside_page and line.startswith(\"<title>\") and line.endswith(\"</title>\"):\n",
    "            title = line[len(\"<title>\"):-len(\"</title>\")]\n",
    "\n",
    "        elif inside_page and \"<text\" in line:\n",
    "            inside_text_tag = True\n",
    "            # Capture content after opening <text> tag (may include attributes)\n",
    "            start = line.find('>') + 1\n",
    "            end = line.find(\"</text>\")\n",
    "            if end != -1:\n",
    "                text.append(line[start:end])\n",
    "                inside_text_tag = False\n",
    "            else:\n",
    "                text.append(line[start:])\n",
    "\n",
    "        elif inside_text_tag:\n",
    "            end = line.find(\"</text>\")\n",
    "            if end != -1:\n",
    "                text.append(line[:end])\n",
    "                inside_text_tag = False\n",
    "            else:\n",
    "                text.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb1c74",
   "metadata": {},
   "source": [
    "## Detecting Redirects\n",
    "\n",
    "Redirects in the Wikipedia dump tend to start with a `#`.\n",
    "For instance, a redirect in the dump can look like:\n",
    "```\n",
    "Redirect_name  -->  #REDIRECT Actual page title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7769b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if title and text:\n",
    "    page_text = '\\n'.join(text).strip()\n",
    "    is_redirect = page_text.lower().startswith(\"#\")\n",
    "    titles_and_texts.append({\n",
    "        \"title\": title,\n",
    "        \"text\": page_text,\n",
    "        \"is_redirect\": is_redirect\n",
    "    })\n",
    "    \n",
    "# print a few of them to check them\n",
    "for entry in titles_and_texts[:100]: # adjust the number here if you want to print less redirects\n",
    "    print(f\"Title: {entry['title']}\")\n",
    "    print(f\"Redirect: {entry.get('is_redirect', False)}\")\n",
    "    print(f\"Text snippet: {entry['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b4e2c9",
   "metadata": {},
   "source": [
    "## Detect pages that you might want to remove\n",
    "\n",
    "Our goal was to remove pages that act as automatic templates or are mostly help pages or list of links. To detect pages that adhere to this type you need to perform a manual check of titles.\n",
    "\n",
    "What we noticed when we were building WikiTextGraph was that pages that have a colon (\":\") usually fit to the description above. These titles can vary from language to language, thus it can take some time to spot them.\n",
    "\n",
    "Below we provide an example to see how could you look for pages like these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in titles_and_texts:\n",
    "    if \"Mediawiki:\" in entry[\"title\"]: # here you can change \"Mediawiki\" with something else\n",
    "        # refer to LANG_SETTINGS.yml for some examples in multiple language versions\n",
    "        print(\"Title: \", entry[\"title\"])\n",
    "        print(\"Text: \", entry[\"text\"][:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikitextgraph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

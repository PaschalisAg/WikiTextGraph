{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c10f846",
   "metadata": {},
   "source": [
    "1. **Entry Point** (`wikitextgraph.py`)\n",
    "- Parses command-line arguments (`--dump_filepath`, `--language_code`, `--base_dir`, `--generate_graph`).\n",
    "- If required inputs are missing, launches a GUI prompt via `gui_prompt_for_inputs()`.\n",
    "- Ensures the base output directory exists.\n",
    "- Calls `parse_wikidump()` from `parser_module`, passing file path, language settings, and graph flag.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Configuration** (`config.py` & `LANG_SETTINGS.yml`)\n",
    "- `LANG_SETTINGS.yml`: Stores language-specific regex patterns for section headers, page filters, and redirect keywords for 10 languages (en, es, el, pl, it, nl, eu, hi, de, vi).\n",
    "- `config.py`: \n",
    "    - `load_language_settings()`: Reads and compiles regex patterns from YAML.\n",
    "    - `get_language_settings()`: Returns settings for a given language code (defaults to English).\n",
    "\n",
    "---\n",
    "\n",
    "3. **Parsing and Processing** (`parser_module.py`)\n",
    "\n",
    "An `xml.sax` handler that:\n",
    "\n",
    "- Buffers `<title>` and `<text>` elements in batches (default 10k pages).\n",
    "\n",
    "- On each batch end, creates a DataFrame, filters non-content pages, extracts main text via `utils.extract_wiki_main_text()`, and writes to a Parquet file with gzip compression.\n",
    "\n",
    "- Cleans up memory with `gc.collect()` between batches.\n",
    "\n",
    "- Loads language settings (section delimiter regex, filter patterns, redirect keywords).\n",
    "\n",
    "- Sets up output directories and files (e.g., `en/output/en_WP_titles_texts.parquet`). \n",
    "\n",
    "- Instantiates WikiXmlHandler, streams the `.bz2` dump through SAX.\n",
    "\n",
    "- After parsing, optionally calls `generate_graph()` to build the link graph.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Text Cleaning and Utilities** (`utils.py`)\n",
    "\n",
    "- Template and Tag Removal: Strips `{{templates}}`, `<ref>` tags, HTML comments, and trims to main content (starting at first bold text and stopping at \"See also\" sections).\n",
    "\n",
    "- Excludes pages whose titles match specified patterns (e.g., namespaces: Category:, Template:, disambiguation).\n",
    "\n",
    "- Link Extraction Helpers: Extracts wikilinks with regex, fixes underscores in titles, and resolves redirects using a provided mapping.\n",
    "\n",
    "---\n",
    "\n",
    "5. **GUI Interface** (`gui.py`)\n",
    "\n",
    "- A Tkinter-based window prompting users to:\n",
    "  1. Select a compressed `.bz2` dump file.\n",
    "  2. Choose a language code from a dropdown.\n",
    "  3. Decide whether to generate the graph.\n",
    "  4. Select an output directory.\n",
    "- Buttons to open GitHub repo or contact developer.\n",
    "- Returns inputs to `wikitextgraph.py` for processing.\n",
    "\n",
    "---\n",
    "\n",
    "6. **Graph Generation** (`graph.py` - not shown)\n",
    "\n",
    "- Reads the Parquet of titles/texts, extracts links, resolves redirects, and constructs a node-edge Parquet representation.\n",
    "- Outputs in `base_dir/<lang>/graph/`:\n",
    "  - `redirects_rev_mapping.pkl.gzip`\n",
    "  - `<lang>_id_node_mapping.parquet`\n",
    "  - `<lang>_wiki_graph.parquet`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fce42489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import re\n",
    "import pandas as pd\n",
    "from utils import (\n",
    "    extract_wiki_main_text,\n",
    "    filter_non_content_pages,\n",
    "    extract_wikilinks,\n",
    "    fix_dubious_links,\n",
    "    resolve_redirects\n",
    ")\n",
    "\n",
    "# Section headers to stop main text extraction\n",
    "section_patt = re.compile(\n",
    "    r\"(==\\s*(See also|Publications|References|Notes|Footnotes|External links|Further reading)\\s*==|WP:SEEALSO)\"\n",
    ")\n",
    "\n",
    "class TestWikiUtils(unittest.TestCase):\n",
    "\n",
    "    def test_extract_wiki_main_text(self):\n",
    "        \"\"\"\n",
    "        Test that extract_wiki_main_text removes refs/comments/templates and truncates at known end sections.\n",
    "        \"\"\"\n",
    "        sample_text = \"\"\"\n",
    "        <!-- Comment about the article -->\n",
    "        {{Infobox}}\n",
    "        '''Python''' is a high-level programming language. <ref>Reference here</ref>\n",
    "        It was created by [[Guido van Rossum|Guido]].\n",
    "        == References ==\n",
    "        <ref>Extra ref</ref>\n",
    "        \"\"\"\n",
    "        expected_output = \"'''Python''' is a high-level programming language. It was created by [[Guido van Rossum|Guido]].\"\n",
    "        result = extract_wiki_main_text(sample_text, section_patt)\n",
    "        self.assertEqual(\" \".join(result.split()), \" \".join(expected_output.split()))\n",
    "\n",
    "    def test_filter_non_content_pages(self):\n",
    "        \"\"\"\n",
    "        Test that filter_non_content_pages removes pages matching a namespace pattern.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame({\n",
    "            'title': ['Article1', 'User:Example', 'Talk:Article2', 'Main Article'],\n",
    "            'text': ['some text'] * 4\n",
    "        })\n",
    "        patterns = ['^user:', '^talk:']\n",
    "        filtered_df = filter_non_content_pages(df, patterns, redirect_keywords=[])\n",
    "        expected_titles = ['Article1', 'Main Article']\n",
    "        self.assertListEqual(list(filtered_df['title']), expected_titles)\n",
    "\n",
    "    def test_extract_wikilinks(self):\n",
    "        \"\"\"\n",
    "        Test that extract_wikilinks extracts internal Wikipedia links.\n",
    "        \"\"\"\n",
    "        text = \"This links to [[Python (programming language)]] and [[Guido van Rossum|Guido]].\"\n",
    "        wiki_link_regex = re.compile(r'\\[\\[(?:[^|\\]]*\\|)?([^\\]]+)\\]\\]')\n",
    "        links = extract_wikilinks(wiki_link_regex, text)\n",
    "        self.assertListEqual(links, ['Python (programming language)', 'Guido'])\n",
    "\n",
    "    def test_fix_dubious_links(self):\n",
    "        \"\"\"\n",
    "        Test that underscores are replaced with spaces in links.\n",
    "        \"\"\"\n",
    "        self.assertEqual(fix_dubious_links(\"Python_(programming_language)\"), \"Python (programming language)\")\n",
    "        self.assertEqual(fix_dubious_links(\"Guido_van_Rossum\"), \"Guido van Rossum\")\n",
    "        self.assertIsNone(fix_dubious_links(None))\n",
    "\n",
    "    def test_resolve_redirects(self):\n",
    "        \"\"\"\n",
    "        Test that resolve_redirects replaces known redirects.\n",
    "        \"\"\"\n",
    "        series = pd.Series([\"PyLang\", \"Guido\", \"Monty Python\"])\n",
    "        redirect_map = {\n",
    "            \"PyLang\": \"Python (programming language)\",\n",
    "            \"Guido\": \"Guido van Rossum\"\n",
    "        }\n",
    "        resolved = resolve_redirects(series, redirect_map)\n",
    "        expected = pd.Series([\"Python (programming language)\", \"Guido van Rossum\", \"Monty Python\"])\n",
    "        pd.testing.assert_series_equal(resolved, expected)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=[\"first-arg-is-ignored\"], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikitextgraph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
